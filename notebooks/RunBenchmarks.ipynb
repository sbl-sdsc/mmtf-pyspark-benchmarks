{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark for Reading and Datamining PDB Structures with mmtf-pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from mmtfPyspark.io import mmtfReader\n",
    "from mmtfPyspark.interactions import InteractionExtractor, InteractionFilter\n",
    "\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the benchmark\n",
    "Set the path to the MMTF Hadoop Sequence file. Here we retrieve the value of the environment variable MMTF_FULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop Sequence file path: MMTF_FULL=/Users/peter/MMTF_Files/full\n"
     ]
    }
   ],
   "source": [
    "path = mmtfReader.get_mmtf_full_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify a list with the number of cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cores = [4, 2, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create results directory\n",
    "results_dir = '../results'\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Benchmark\n",
    "Benchmarks reading an MMTF Hadoop Sequence File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(path, num_cores):\n",
    "    spark = SparkSession.builder.master(\"local[\" + str(num_cores) + \"]\").appName(\"Read\").getOrCreate()\n",
    "    structures = mmtfReader.read_sequence_file(path)\n",
    "    count = structures.count()\n",
    "    spark.stop()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read, cores: 4 time: 252.54657816886902 seconds\n",
      "read, cores: 2 time: 280.9554250240326 seconds\n",
      "read, cores: 1 time: 433.2996151447296 seconds\n"
     ]
    }
   ],
   "source": [
    "df_read = pd.DataFrame(columns=('cores', 'read'))\n",
    "\n",
    "for num_cores in cores:\n",
    "    start = time.time()\n",
    "    count = read(path, num_cores)\n",
    "    end = time.time()\n",
    "    print('read, cores:', num_cores, 'time:', end-start, 'seconds')\n",
    "    df_read = df_read.append([{'cores':num_cores, 'read': end-start, 'count': count}], ignore_index=True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read.to_csv(os.path.join(results_dir, 'read.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cores</th>\n",
       "      <th>count</th>\n",
       "      <th>read</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>140825.0</td>\n",
       "      <td>252.546578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>140825.0</td>\n",
       "      <td>280.955425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>140825.0</td>\n",
       "      <td>433.299615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cores     count        read\n",
       "0     4  140825.0  252.546578\n",
       "1     2  140825.0  280.955425\n",
       "2     1  140825.0  433.299615"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactions Benchmark\n",
    "This benchmark finds all zinc interactions in PDB structures. Structures with multiple models, e.g., NMR structures are excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactions(path, num_cores):\n",
    "    spark = SparkSession.builder.master(\"local[\" + str(num_cores) + \"]\").appName(\"Interactions\").getOrCreate()\n",
    "    structures = mmtfReader.read_sequence_file(path)\n",
    "    structures = structures.filter(lambda s: s[1].num_models == 1)\n",
    "                               \n",
    "    interaction_filter = InteractionFilter()\n",
    "    interaction_filter.set_target_elements(False, ['C','H','P'])\n",
    "    interaction_filter.set_query_elements(True, ['Zn'])\n",
    "    interaction_filter.set_distance_cutoff(3.0)\n",
    "\n",
    "    interactions = InteractionExtractor().get_ligand_polymer_interactions(structures, interaction_filter)\n",
    "    count = interactions.count()\n",
    "\n",
    "    spark.stop()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interactions, cores: 4 time: 621.5304439067841 seconds\n",
      "interactions, cores: 2 time: 660.8002190589905 seconds\n",
      "interactions, cores: 1 time: 1153.2656798362732 seconds\n"
     ]
    }
   ],
   "source": [
    "df_interactions = pd.DataFrame(columns=('cores', 'interactions'))\n",
    "\n",
    "for num_cores in cores:\n",
    "    start = time.time()\n",
    "    count = interactions(path, num_cores)\n",
    "    end = time.time()\n",
    "    print('interactions, cores:', num_cores, 'time:', end-start, 'seconds')\n",
    "    df_interactions = df_interactions.append([{'cores':num_cores, 'interactions': end-start, 'count': count}], ignore_index=True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interactions.to_csv(os.path.join(results_dir, 'interactions.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cores</th>\n",
       "      <th>count</th>\n",
       "      <th>interactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>127269.0</td>\n",
       "      <td>621.530444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>127269.0</td>\n",
       "      <td>660.800219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>127269.0</td>\n",
       "      <td>1153.265680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cores     count  interactions\n",
       "0     4  127269.0    621.530444\n",
       "1     2  127269.0    660.800219\n",
       "2     1  127269.0   1153.265680"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saltbridges Benchmark\n",
    "This benchmark finds salt bridges in protein structures. Structures with multiple models, e.g., NMR structures are excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saltbridges(path, num_cores):\n",
    "    spark = SparkSession.builder.master(\"local[\" + str(num_cores) + \"]\").appName(\"Saltbridges\").getOrCreate()\n",
    "    structures = mmtfReader.read_sequence_file(path)\n",
    "    structures = structures.filter(lambda s: s[1].num_models == 1)\n",
    "                               \n",
    "    salt_bridge = InteractionFilter(distanceCutoff=3.5)\n",
    "    salt_bridge.set_query_groups(True, ['ASP', 'GLU'])\n",
    "    salt_bridge.set_query_atom_names(True, ['OD1', 'OD2', 'OE1', 'OE2'])\n",
    "    salt_bridge.set_target_groups(True, ['ARG', 'LYS', 'HIS'])\n",
    "    salt_bridge.set_target_atom_names(True, ['NH1', 'NH2', 'NZ', 'ND1', 'NE2'])\n",
    "\n",
    "    interactions = InteractionExtractor.get_polymer_interactions(structures, salt_bridge)\n",
    "    count = interactions.count()\n",
    "\n",
    "    spark.stop()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saltbridges, cores: 4 time: 1061.849042892456 seconds\n",
      "saltbridges, cores: 2 time: 1114.9261240959167 seconds\n",
      "saltbridges, cores: 1 time: 2019.266900062561 seconds\n"
     ]
    }
   ],
   "source": [
    "df_saltbridges = pd.DataFrame(columns=('cores', 'saltbridges'))\n",
    "\n",
    "for num_cores in cores:\n",
    "    start = time.time()\n",
    "    count = saltbridges(path, num_cores)\n",
    "    end = time.time()\n",
    "    print('saltbridges, cores:', num_cores, 'time:', end-start, 'seconds')\n",
    "    df_saltbridges = df_saltbridges.append([{'cores':num_cores, 'saltbridges': end-start, 'count': count}], ignore_index=True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cores</th>\n",
       "      <th>count</th>\n",
       "      <th>saltbridges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>523868.0</td>\n",
       "      <td>1061.849043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>523868.0</td>\n",
       "      <td>1114.926124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>523868.0</td>\n",
       "      <td>2019.266900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cores     count  saltbridges\n",
       "0     4  523868.0  1061.849043\n",
       "1     2  523868.0  1114.926124\n",
       "2     1  523868.0  2019.266900"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_saltbridges.to_csv(os.path.join(results_dir, 'saltbridges.csv'), index=False)\n",
    "df_saltbridges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read MMTF Hadoop Sequence File\n",
    "This benchmarks read the raw MMTF Hadoop Sequence file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hadoop(path, num_cores):\n",
    "    spark = SparkSession.builder.master(\"local[\" + str(num_cores) + \"]\").appName(\"Read\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    text = \"org.apache.hadoop.io.Text\"\n",
    "    byteWritable = \"org.apache.hadoop.io.BytesWritable\"\n",
    "    rdd = sc.sequenceFile(path, text, byteWritable)\n",
    "    count = rdd.count()\n",
    "    spark.stop()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_hadoop, cores: 4 time: 68.62874412536621 seconds\n",
      "read_hadoop, cores: 2 time: 73.14868211746216 seconds\n",
      "read_hadoop, cores: 1 time: 107.54851508140564 seconds\n"
     ]
    }
   ],
   "source": [
    "df_read_hadoop = pd.DataFrame(columns=('cores', 'read_hadoop'))\n",
    "\n",
    "for num_cores in cores:\n",
    "    start = time.time()\n",
    "    count = read_hadoop(path, num_cores)\n",
    "    end = time.time()\n",
    "    print('read_hadoop, cores:', num_cores, 'time:', end-start, 'seconds')\n",
    "    df_read_hadoop = df_read_hadoop.append([{'cores':num_cores, 'read_hadoop': end-start, 'count': count}], ignore_index=True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cores</th>\n",
       "      <th>count</th>\n",
       "      <th>read_hadoop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>140825.0</td>\n",
       "      <td>68.628744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>140825.0</td>\n",
       "      <td>73.148682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>140825.0</td>\n",
       "      <td>107.548515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cores     count  read_hadoop\n",
       "0     4  140825.0    68.628744\n",
       "1     2  140825.0    73.148682\n",
       "2     1  140825.0   107.548515"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_read_hadoop.to_csv(os.path.join(results_dir, 'read_hadoop.csv'), index=False)\n",
    "df_read_hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and and Unzip Data in MMTF Hadoop Sequence File\n",
    "This benchmark reads the MMTF Hadoop Sequence File and unzips the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip(path, num_cores):\n",
    "    spark = SparkSession.builder.master(\"local[\" + str(num_cores) + \"]\").appName(\"Read\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    text = \"org.apache.hadoop.io.Text\"\n",
    "    byteWritable = \"org.apache.hadoop.io.BytesWritable\"\n",
    "    rdd = sc.sequenceFile(path, text, byteWritable)  ## returns key/value tuples\n",
    "    data = rdd.map(lambda t: gzip.decompress(t[1]))  # t[1] are the values in the rdd\n",
    "    count = data.count()\n",
    "    spark.stop()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzip, cores: 4 time: 100.05307197570801 seconds\n",
      "unzip, cores: 2 time: 110.18872785568237 seconds\n",
      "unzip, cores: 1 time: 167.66900515556335 seconds\n"
     ]
    }
   ],
   "source": [
    "df_unzip = pd.DataFrame(columns=('cores', 'unzip'))\n",
    "\n",
    "for num_cores in cores:\n",
    "    start = time.time()\n",
    "    count = unzip(path, num_cores)\n",
    "    end = time.time()\n",
    "    print('unzip, cores:', num_cores, 'time:', end-start, 'seconds')\n",
    "    df_unzip = df_unzip.append([{'cores':num_cores, 'unzip': end-start, 'count': count}], ignore_index=True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cores</th>\n",
       "      <th>count</th>\n",
       "      <th>unzip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>140825.0</td>\n",
       "      <td>100.053072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>140825.0</td>\n",
       "      <td>110.188728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>140825.0</td>\n",
       "      <td>167.669005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cores     count       unzip\n",
       "0     4  140825.0  100.053072\n",
       "1     2  140825.0  110.188728\n",
       "2     1  140825.0  167.669005"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unzip.to_csv(os.path.join(results_dir, 'unzip.csv'), index=False)\n",
    "df_unzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpack Data\n",
    "This benchmark read an MMTF Hadoop Sequence File, unzips the data, and decodes the data using the Pandas libarary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_pd(path, num_cores):\n",
    "    spark = SparkSession.builder.master(\"local[\" + str(num_cores) + \"]\").appName(\"Read\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    text = \"org.apache.hadoop.io.Text\"\n",
    "    byteWritable = \"org.apache.hadoop.io.BytesWritable\"\n",
    "    rdd = sc.sequenceFile(path, text, byteWritable)  ## returns key/value tuples\n",
    "    data = rdd.map(lambda t: gzip.decompress(t[1]))  # t[1] are the values in the rdd\n",
    "    unpack = data.map(lambda d: pd.read_msgpack(d))\n",
    "    count = unpack.count()\n",
    "    spark.stop()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unpack_pd, cores: 4 time: 129.77199506759644 seconds\n",
      "unpack_pd, cores: 2 time: 146.0845558643341 seconds\n",
      "unpack_pd, cores: 1 time: 226.78590202331543 seconds\n"
     ]
    }
   ],
   "source": [
    "df_unpack_pd = pd.DataFrame(columns=('cores', 'unpack_pd'))\n",
    "\n",
    "for num_cores in cores:\n",
    "    start = time.time()\n",
    "    count = unpack_pd(path, num_cores)\n",
    "    end = time.time()\n",
    "    print('unpack_pd, cores:', num_cores, 'time:', end-start, 'seconds')\n",
    "    df_unpack_pd = df_unpack_pd.append([{'cores':num_cores, 'unpack_pd': end-start, 'count': count}], ignore_index=True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cores</th>\n",
       "      <th>count</th>\n",
       "      <th>unpack_pd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>140825.0</td>\n",
       "      <td>129.771995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>140825.0</td>\n",
       "      <td>146.084556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>140825.0</td>\n",
       "      <td>226.785902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cores     count   unpack_pd\n",
       "0     4  140825.0  129.771995\n",
       "1     2  140825.0  146.084556\n",
       "2     1  140825.0  226.785902"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unpack_pd.to_csv(os.path.join(results_dir, 'unpack_pd.csv'), index=False)\n",
    "df_unpack_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpack Data using MsgPack\n",
    "This benchmark read an MMTF Hadoop Sequence File, unzips the data, and decodes the data using the msgpack library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import msgpack\n",
    "def unpack_msgpack(path, num_cores):\n",
    "    spark = SparkSession.builder.master(\"local[\" + str(num_cores) + \"]\").appName(\"Read\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    text = \"org.apache.hadoop.io.Text\"\n",
    "    byteWritable = \"org.apache.hadoop.io.BytesWritable\"\n",
    "    rdd = sc.sequenceFile(path, text, byteWritable)  ## returns key/value tuples\n",
    "    data = rdd.map(lambda t: gzip.decompress(t[1]))  # t[1] are the values in the rdd\n",
    "    unpack = data.map(lambda d: msgpack.unpackb(d, raw=False))\n",
    "    count = unpack.count()\n",
    "    spark.stop()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unpack_msgpack, cores: 4 time: 439.6972670555115 seconds\n",
      "unpack_msgpack, cores: 2 time: 439.4969379901886 seconds\n",
      "unpack_msgpack, cores: 1 time: 777.9126970767975 seconds\n"
     ]
    }
   ],
   "source": [
    "df_unpack_msgpack = pd.DataFrame(columns=('cores', 'unpack_msgpack'))\n",
    "\n",
    "for num_cores in cores:\n",
    "    start = time.time()\n",
    "    count = unpack_msgpack(path, num_cores)\n",
    "    end = time.time()\n",
    "    print('unpack_msgpack, cores:', num_cores, 'time:', end-start, 'seconds')\n",
    "    df_unpack_msgpack = df_unpack_msgpack.append([{'cores':num_cores, 'unpack_msgpack': end-start, 'count': count}], ignore_index=True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cores</th>\n",
       "      <th>count</th>\n",
       "      <th>unpack_msgpack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>140825.0</td>\n",
       "      <td>439.697267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>140825.0</td>\n",
       "      <td>439.496938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>140825.0</td>\n",
       "      <td>777.912697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cores     count  unpack_msgpack\n",
       "0     4  140825.0      439.697267\n",
       "1     2  140825.0      439.496938\n",
       "2     1  140825.0      777.912697"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unpack_msgpack.to_csv(os.path.join(results_dir, 'unpack_msgpack.csv'), index=False)\n",
    "df_unpack_msgpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
